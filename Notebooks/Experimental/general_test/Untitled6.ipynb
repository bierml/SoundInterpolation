{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IT_J_VqNG-kt",
        "outputId": "8f84bb4a-e4ce-4fc9-a47a-15740c5e934f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ spectrogram_model_layer_6            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m400,335\u001b[0m │\n",
              "│ (\u001b[38;5;33mSpectrogramModelLayer\u001b[0m)              │                             │                 │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ spectrogram_model_layer_6            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">400,335</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpectrogramModelLayer</span>)              │                             │                 │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m400,335\u001b[0m (1.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">400,335</span> (1.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m400,335\u001b[0m (1.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">400,335</span> (1.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels: 1, Sample Width: 2, Frame Rate: 44100, Frames: 122880\n",
            "Channels: 1, Sample Width: 2, Frame Rate: 44100, Frames: 122880\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 2/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 8.0640e-04 - mse: 8.0640e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 8.7661e-04 - mse: 8.7661e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 7.9961e-04 - mse: 7.9961e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 5.8451e-04 - mse: 5.8451e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 4.4957e-04 - mse: 4.4957e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 4.5085e-04 - mse: 4.5085e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 5.1859e-04 - mse: 5.1859e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 3.5075e-04 - mse: 3.5075e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 3.5863e-04 - mse: 3.5863e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 3.0617e-04 - mse: 3.0617e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 2.8196e-04 - mse: 2.8196e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 3.2068e-04 - mse: 3.2068e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 3.1774e-04 - mse: 3.1774e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 144ms/step - loss: 3.4377e-04 - mse: 3.4377e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 3.0118e-04 - mse: 3.0118e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 3.1562e-04 - mse: 3.1562e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 2.6489e-04 - mse: 2.6489e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 3.0209e-04 - mse: 3.0209e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 2.1458e-04 - mse: 2.1458e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 144ms/step - loss: 1.9101e-04 - mse: 1.9101e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.8054e-04 - mse: 1.8054e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1.6063e-04 - mse: 1.6063e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.6644e-04 - mse: 1.6644e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 1.9771e-04 - mse: 1.9771e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1.6998e-04 - mse: 1.6998e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1.5567e-04 - mse: 1.5567e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 130ms/step - loss: 1.4310e-04 - mse: 1.4310e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 1.4138e-04 - mse: 1.4138e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 1.8187e-04 - mse: 1.8187e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - loss: 1.5355e-04 - mse: 1.5355e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.5951e-04 - mse: 1.5951e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1.5186e-04 - mse: 1.5186e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 1.3143e-04 - mse: 1.3143e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.4370e-04 - mse: 1.4370e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 1.2893e-04 - mse: 1.2893e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 1.2936e-04 - mse: 1.2936e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 1.1281e-04 - mse: 1.1281e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1.3271e-04 - mse: 1.3271e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.4321e-04 - mse: 1.4321e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 9.9047e-05 - mse: 9.9047e-05\n",
            "Epoch 43/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 1.0613e-04 - mse: 1.0613e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.3594e-04 - mse: 1.3594e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.6492e-04 - mse: 1.6492e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 1.5977e-04 - mse: 1.5977e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 1.6006e-04 - mse: 1.6006e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 1.2635e-04 - mse: 1.2635e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 155ms/step - loss: 1.3645e-04 - mse: 1.3645e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - loss: 1.1698e-04 - mse: 1.1698e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 1.4959e-04 - mse: 1.4959e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 1.1146e-04 - mse: 1.1146e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 1.1490e-04 - mse: 1.1490e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 1.2983e-04 - mse: 1.2983e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - loss: 1.2591e-04 - mse: 1.2591e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.0187e-04 - mse: 1.0187e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.4444e-04 - mse: 1.4444e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - loss: 1.1107e-04 - mse: 1.1107e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - loss: 1.2430e-04 - mse: 1.2430e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1.4953e-04 - mse: 1.4953e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.1118e-04 - mse: 1.1118e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 133ms/step - loss: 1.1627e-04 - mse: 1.1627e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 9.4626e-05 - mse: 9.4626e-05\n",
            "Epoch 64/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1.0677e-04 - mse: 1.0677e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - loss: 1.1411e-04 - mse: 1.1411e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 9.7918e-05 - mse: 9.7918e-05\n",
            "Epoch 67/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 7.1521e-05 - mse: 7.1521e-05\n",
            "Epoch 68/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - loss: 1.2680e-04 - mse: 1.2680e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 129ms/step - loss: 8.4062e-05 - mse: 8.4062e-05\n",
            "Epoch 70/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 9.1049e-05 - mse: 9.1049e-05\n",
            "Epoch 71/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1.4282e-04 - mse: 1.4282e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - loss: 1.4678e-04 - mse: 1.4678e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1.0880e-04 - mse: 1.0880e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1.1334e-04 - mse: 1.1334e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - loss: 1.5789e-04 - mse: 1.5789e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 9.1994e-05 - mse: 9.1994e-05\n",
            "Epoch 77/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 9.2187e-05 - mse: 9.2187e-05\n",
            "Epoch 78/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 9.9652e-05 - mse: 9.9652e-05\n",
            "Epoch 79/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 1.1159e-04 - mse: 1.1159e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 9.2610e-05 - mse: 9.2610e-05\n",
            "Epoch 81/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 1.0703e-04 - mse: 1.0703e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - loss: 8.6006e-05 - mse: 8.6006e-05\n",
            "Epoch 83/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 1.0809e-04 - mse: 1.0809e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 1.0270e-04 - mse: 1.0270e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 141ms/step - loss: 8.4276e-05 - mse: 8.4276e-05\n",
            "Epoch 86/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 1.0289e-04 - mse: 1.0289e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 8.2284e-05 - mse: 8.2284e-05\n",
            "Epoch 88/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 8.8969e-05 - mse: 8.8969e-05\n",
            "Epoch 89/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 9.8202e-05 - mse: 9.8202e-05\n",
            "Epoch 90/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 1.1660e-04 - mse: 1.1660e-04\n",
            "Epoch 91/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 1.2995e-04 - mse: 1.2995e-04\n",
            "Epoch 92/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 7.8896e-05 - mse: 7.8896e-05\n",
            "Epoch 93/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 146ms/step - loss: 7.5075e-05 - mse: 7.5075e-05\n",
            "Epoch 94/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 8.6350e-05 - mse: 8.6350e-05\n",
            "Epoch 95/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 8.6577e-05 - mse: 8.6577e-05\n",
            "Epoch 96/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 147ms/step - loss: 1.0190e-04 - mse: 1.0190e-04\n",
            "Epoch 97/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 9.9299e-05 - mse: 9.9299e-05\n",
            "Epoch 98/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 8.2998e-05 - mse: 8.2998e-05\n",
            "Epoch 99/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 147ms/step - loss: 7.5132e-05 - mse: 7.5132e-05\n",
            "Epoch 100/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 8.1221e-05 - mse: 8.1221e-05\n",
            "Channels: 1, Sample Width: 2, Frame Rate: 44100, Frames: 122880\n",
            "(122752,)\n",
            "WAV file written to output.wav\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Experimental.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1sakgMfi7rWd_pO6fIB1zYbAVN1TlxzGk\n",
        "\n",
        "Открываем исходный файл и файл с клиппингом.\n",
        "\n",
        "*   wav_file_path - путь до исходного файла (без клиппинга), его данные записываются в массив samples\n",
        "*   wav_file_path1 - путь до файла с клиппингом, его данные записываются в массив samples1\n",
        "\"\"\"\n",
        "wav_file_path = \"1.wav\"\n",
        "wav_file_path1 = \"1c.wav\"\n",
        "import wave\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "def read_wav_as_float(file_path):\n",
        "    \"\"\"\n",
        "    Reads a WAV file and returns its samples as a NumPy array of float32 values.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the WAV file.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An array of float32 samples in the range [-1.0, 1.0].\n",
        "    \"\"\"\n",
        "    with wave.open(file_path, 'rb') as wav_file:\n",
        "        # Get parameters\n",
        "        n_channels = wav_file.getnchannels()\n",
        "        sample_width = wav_file.getsampwidth()\n",
        "        n_frames = wav_file.getnframes()\n",
        "        frame_rate = wav_file.getframerate()\n",
        "        print(f\"Channels: {n_channels}, Sample Width: {sample_width}, Frame Rate: {frame_rate}, Frames: {n_frames}\")\n",
        "\n",
        "        # Read frames as bytes\n",
        "        raw_data = wav_file.readframes(n_frames)\n",
        "\n",
        "    # Determine the data type based on sample width\n",
        "    dtype = {1: np.int8, 2: np.int16, 4: np.int32}.get(sample_width)\n",
        "    if dtype is None:\n",
        "        raise ValueError(f\"Unsupported sample width: {sample_width}\")\n",
        "\n",
        "    # Convert raw bytes to numpy array without copying data\n",
        "    int_data = np.frombuffer(raw_data, dtype=dtype)\n",
        "\n",
        "    # Convert to float32 and normalize to range [-1.0, 1.0]\n",
        "    max_val = float(2 ** (8 * sample_width - 1))\n",
        "    float_data = int_data.astype(np.float32) / max_val\n",
        "\n",
        "    # Handle multi-channel audio by averaging channels\n",
        "    if n_channels > 1:\n",
        "        float_data = float_data.reshape(-1, n_channels).mean(axis=1)\n",
        "\n",
        "    return float_data\n",
        "\n",
        "\"\"\"Функция для записи массива в файл по пути output_path.\"\"\"\n",
        "\n",
        "def write_float_samples_to_wav(samples, sample_rate, output_path):\n",
        "    \"\"\"\n",
        "    Writes floating-point audio samples to a mono 16-bit WAV file.\n",
        "\n",
        "    Parameters:\n",
        "        samples (list or np.ndarray): Array of floating-point audio samples in the range [-1.0, 1.0].\n",
        "        sample_rate (int): Sample rate of the audio in Hz (e.g., 44100).\n",
        "        output_path (str): Path to save the output WAV file.\n",
        "    \"\"\"\n",
        "    # Ensure the samples are a NumPy array\n",
        "    samples = np.array(samples, dtype=np.float32)\n",
        "\n",
        "    # Clip the samples to the range [-1.0, 1.0] to prevent overflow\n",
        "    samples = np.clip(samples, -1.0, 1.0)\n",
        "\n",
        "    # Convert to 16-bit PCM format\n",
        "    int_samples = (samples * 32767).astype(np.int16)\n",
        "\n",
        "    # Write to a WAV file\n",
        "    with wave.open(output_path, 'wb') as wav_file:\n",
        "        # Set the parameters for the WAV file\n",
        "        wav_file.setnchannels(1)  # Mono\n",
        "        wav_file.setsampwidth(2)  # 16-bit PCM\n",
        "        wav_file.setframerate(sample_rate)\n",
        "\n",
        "        # Write the audio frames\n",
        "        wav_file.writeframes(int_samples.tobytes())\n",
        "\n",
        "class AudioDataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    A Keras Sequence for generating batches of overlapping audio sequences.\n",
        "\n",
        "    This generator loads the entire audio files once, computes the starting indices\n",
        "    for sequences of length SQNC_LENGTH with 50% overlap, and yields batches of data.\n",
        "    \"\"\"\n",
        "    def __init__(self, original_file, clipped_file, SQNC_LENGTH, batch_size=32, shuffle=True):\n",
        "        self.samples = read_wav_as_float(original_file)\n",
        "        self.samples_clipped = read_wav_as_float(clipped_file)\n",
        "        self.SQNC_LENGTH = SQNC_LENGTH\n",
        "        self.batch_size = batch_size\n",
        "        self.step_size = SQNC_LENGTH // 2  # 50% overlap\n",
        "        # Compute starting indices for sequences\n",
        "        self.indices = list(range(0, len(self.samples) - SQNC_LENGTH + 1, self.step_size))\n",
        "        self.shuffle = shuffle\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of batches per epoch is the total number of sequences divided by batch_size.\n",
        "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        X_batch = []\n",
        "        y_batch = []\n",
        "        for i in batch_indices:\n",
        "            X_batch.append(self.samples_clipped[i : i + self.SQNC_LENGTH])\n",
        "            y_batch.append(self.samples[i : i + self.SQNC_LENGTH])\n",
        "        return np.array(X_batch), np.array(y_batch)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "\"\"\"Обучение нейросети на множестве спектрограмм сигнала. N и M - количество точек по осям частоты и времени соответственно в обучающих выборках.\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "FSTEP = 16\n",
        "# Custom STFT layer using tf.signal.stft\n",
        "class STFTLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, frame_length=8, frame_step=4, **kwargs):\n",
        "        super(STFTLayer, self).__init__(**kwargs)\n",
        "        self.frame_length = frame_length\n",
        "        self.frame_step = frame_step\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: shape (batch, sq_lngth)\n",
        "        # Use a Hann window\n",
        "        window = tf.signal.hann_window(self.frame_length, dtype=inputs.dtype)\n",
        "        stft_result = tf.signal.stft(\n",
        "            inputs,\n",
        "            frame_length=self.frame_length,\n",
        "            frame_step=self.frame_step,\n",
        "            window_fn=lambda fl, dtype: window\n",
        "        )\n",
        "        # tf.signal.stft returns shape (batch, time_frames, fft_unique_bins)\n",
        "        # For our design, we want to use (batch, fft_unique_bins, time_frames)\n",
        "        magnitude = tf.abs(stft_result)\n",
        "        phase = tf.math.angle(stft_result)\n",
        "        # Transpose to shape (batch, fft_unique_bins, time_frames)\n",
        "        magnitude = tf.transpose(magnitude, perm=[0, 2, 1])\n",
        "        phase = tf.transpose(phase, perm=[0, 2, 1])\n",
        "        return magnitude, phase\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch = input_shape[0]\n",
        "        if input_shape[1] is None:\n",
        "            return (batch, None, None), (batch, None, None)\n",
        "        # time_frames computed from signal length:\n",
        "        time_frames = (input_shape[1] - self.frame_length) // self.frame_step + 1\n",
        "        fft_bins = self.frame_length // 2 + 1\n",
        "        # After transposition, output shape becomes (batch, fft_bins, time_frames)\n",
        "        return (batch, fft_bins, time_frames), (batch, fft_bins, time_frames)\n",
        "\n",
        "\n",
        "# Custom inverse STFT layer using tf.signal.inverse_stft\n",
        "class ISTFTLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, frame_length=8, frame_step=4, sq_lngth=None, **kwargs):\n",
        "        super(ISTFTLayer, self).__init__(**kwargs)\n",
        "        self.frame_length = frame_length\n",
        "        self.frame_step = frame_step\n",
        "        self.sq_lngth = sq_lngth\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: a list [mag, phase] with shapes (batch, F, T)\n",
        "        mag, phase = inputs\n",
        "        # tf.signal.inverse_stft expects input of shape (batch, time_frames, fft_unique_bins).\n",
        "        # So transpose mag and phase from (batch, F, T) to (batch, T, F):\n",
        "        mag_t = tf.transpose(mag, perm=[0, 2, 1])\n",
        "        phase_t = tf.transpose(phase, perm=[0, 2, 1])\n",
        "        stft_complex = tf.cast(mag_t, tf.complex64) * tf.exp(1j * tf.cast(phase_t, tf.complex64))\n",
        "        window = tf.signal.hann_window(self.frame_length, dtype=tf.float32)\n",
        "        reconstructed = tf.signal.inverse_stft(\n",
        "            stft_complex,\n",
        "            frame_length=self.frame_length,\n",
        "            frame_step=self.frame_step,\n",
        "            window_fn=lambda fl, dtype: window\n",
        "        )\n",
        "        if self.sq_lngth is not None:\n",
        "            reconstructed = reconstructed[:, :self.sq_lngth]\n",
        "        return reconstructed\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch = input_shape[0][0]\n",
        "        if self.sq_lngth is not None:\n",
        "            return (batch, self.sq_lngth)\n",
        "        else:\n",
        "            return (batch, None)\n",
        "\n",
        "\n",
        "# Helper layers to add and remove a singleton channel dimension.\n",
        "class AddInnerDim(tf.keras.layers.Layer):\n",
        "    def call(self, x):\n",
        "        return tf.expand_dims(x, axis=-1)\n",
        "\n",
        "class Squeeze(tf.keras.layers.Layer):\n",
        "    def call(self, x):\n",
        "        return tf.squeeze(x, axis=-1)\n",
        "\n",
        "# Custom layer wrapping the entire spectrogram processing pipeline.\n",
        "class SpectrogramModelLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, sq_lngth, **kwargs):\n",
        "        super(SpectrogramModelLayer, self).__init__(**kwargs)\n",
        "        self.sq_lngth = sq_lngth\n",
        "        self.frame_step = FSTEP\n",
        "        self.frame_length = FSTEP * 2  # same as in original (8 if FSTEP=4)\n",
        "        # Frequency bins: frame_length//2 + 1 = FSTEP+1 (e.g. 5)\n",
        "        self.F_const = self.frame_step + 1\n",
        "        # Time frames computed from signal length (same as original)\n",
        "        self.M_const = (sq_lngth - self.frame_length) // self.frame_step + 1\n",
        "\n",
        "        # Instantiate our custom STFT/ISTFT and helper layers.\n",
        "        self.stft_layer = STFTLayer(frame_length=self.frame_length, frame_step=self.frame_step)\n",
        "        self.istft_layer = ISTFTLayer(frame_length=self.frame_length, frame_step=self.frame_step, sq_lngth=sq_lngth)\n",
        "        self.add_inner = AddInnerDim()\n",
        "        self.squeeze = Squeeze()\n",
        "\n",
        "        # Convolution and RNN layers for processing the magnitude.\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')\n",
        "        # After Conv2D, the tensor shape is (batch, F, T, 64)\n",
        "        # We reshape it to (batch, F, T*64) where F = F_const.\n",
        "        self.rnn1 = tf.keras.layers.SimpleRNN(units=sq_lngth, activation='relu', return_sequences=True)\n",
        "        self.rnn2 = tf.keras.layers.SimpleRNN(units=sq_lngth // 2, activation='relu', return_sequences=True)\n",
        "        self.dense = tf.keras.layers.Dense(units=self.M_const, activation='linear')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: shape (batch, sq_lngth)\n",
        "        # Compute STFT; stft_layer returns a tuple (mag, phase) each with shape (batch, F, T)\n",
        "        mag, phase = self.stft_layer(inputs)\n",
        "\n",
        "        # Crop to M_const time frames (if necessary)\n",
        "        mag = mag[:, :, :self.M_const]\n",
        "        phase = phase[:, :, :self.M_const]\n",
        "\n",
        "        # Add a singleton channel dimension (for Conv2D)\n",
        "        mag = self.add_inner(mag)   # now shape: (batch, F, T, 1)\n",
        "        phase = self.add_inner(phase)  # (batch, F, T, 1)\n",
        "\n",
        "        # Process the magnitude with two Conv2D layers.\n",
        "        x = self.conv1(mag)\n",
        "        x = self.conv2(x)  # shape: (batch, F, T, 64)\n",
        "\n",
        "        # Reshape for RNN processing:\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        # We treat the frequency dimension F as the timesteps (F_const = FSTEP+1)\n",
        "        # and flatten the T (time frames) and channel dimensions.\n",
        "        x = tf.reshape(x, [batch_size, self.F_const, self.M_const * 64])\n",
        "\n",
        "        # Process with two SimpleRNN layers.\n",
        "        x = self.rnn1(x)\n",
        "        x = self.rnn2(x)\n",
        "        # Map each of the F timesteps to M_const outputs.\n",
        "        x = self.dense(x)  # now x has shape (batch, F, M_const)\n",
        "\n",
        "        # Process phase: remove the singleton channel dimension.\n",
        "        phase = self.squeeze(phase)  # shape: (batch, F, M_const)\n",
        "\n",
        "        # Reconstruct the time-domain signal via ISTFT.\n",
        "        reconstructed = self.istft_layer([x, phase])  # shape: (batch, sq_lngth)\n",
        "        # Add a residual connection: original input + reconstruction.\n",
        "        return inputs + reconstructed\n",
        "\n",
        "# Now reimplement build_rnn_spectrogram_model using Sequential.\n",
        "def build_rnn_spectrogram_model(sq_lngth):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(sq_lngth,)),\n",
        "        SpectrogramModelLayer(sq_lngth=sq_lngth)\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mse']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "SQNC_LENGTH = 256\n",
        "# Example usage:\n",
        "# Assume SQNC_LENGTH, samples_sequences_clipped, and samples_sequences are defined.\n",
        "model = build_rnn_spectrogram_model(SQNC_LENGTH)\n",
        "model.summary()\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
        "batch_size = 32\n",
        "train_gen = AudioDataGenerator(wav_file_path, wav_file_path1, SQNC_LENGTH, batch_size=batch_size, shuffle=True)\n",
        "steps_per_epoch = (len(train_gen.samples) - SQNC_LENGTH) // (SQNC_LENGTH // 2 * batch_size)\n",
        "model.fit(train_gen,\n",
        "          epochs=100,\n",
        "          callbacks=[early_stopping])\n",
        "\n",
        "\"\"\"Открытие файла который нужно восстановить и получение массива его спектрограмм. file_for_restoration_path - путь к файлу который нужно восстановить.\n",
        "samples_input_sequences - массив семплов этого файла\n",
        "\n",
        "Zyy,phsy - массивы амплитудных и фазовых спектрограмм файла соответственно\n",
        "\"\"\"\n",
        "\n",
        "file_for_restoration_path = \"1c.wav\"\n",
        "samples_input_file = read_wav_as_float(file_for_restoration_path)\n",
        "j = 0\n",
        "SQNC_LENGTH = 256\n",
        "fs = 44100\n",
        "samples_input_sequences = []\n",
        "while j < len(samples_input_file ):\n",
        "    if(j+SQNC_LENGTH < len(samples_input_file)):\n",
        "        samples_input_sequences.append(samples_input_file[j:j+SQNC_LENGTH])\n",
        "    j += SQNC_LENGTH\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import stft, istft\n",
        "import wave\n",
        "# Open the WAV file\n",
        "with wave.open(file_for_restoration_path, 'rb') as wav_file:\n",
        "    fs = wav_file.getframerate()\n",
        "\n",
        "\n",
        "\"\"\"Для правильного восстановления нужны накладывающиеся последовательности семплов исходного файла. Для простоты возьмем степень наложения окон равной 0.5.\"\"\"\n",
        "\n",
        "#print(samples_restored)\n",
        "restored_samples_overlap = []\n",
        "overlap_input_sequences = []\n",
        "step_size = SQNC_LENGTH // 2\n",
        "j = 0\n",
        "maxv = np.max(np.array(samples_input_file))\n",
        "minv = np.min(np.array(samples_input_file))\n",
        "while j < len(samples_input_file):\n",
        "    #print(j, j+SQNC_LENGTH-1)\n",
        "    if(j+step_size < len(samples_input_file)):\n",
        "        overlap_input_sequences.append(samples_input_file[j:j+SQNC_LENGTH])\n",
        "    j += step_size\n",
        "for sqnc in overlap_input_sequences:\n",
        "  if(max(sqnc)>(maxv*0.95) or min(sqnc)<(minv*0.95)):\n",
        "    elem = np.array(sqnc)\n",
        "    elem = np.expand_dims(elem, axis=0)  # Now shape is (1, SQNC_LENGTH)\n",
        "    res = model.predict(elem,verbose=0).flatten()\n",
        "    #print(res[SQNC_LENGTH//4:(SQNC_LENGTH*3)//4])\n",
        "    restored_samples_overlap.append(res[SQNC_LENGTH//4:(SQNC_LENGTH*3)//4])\n",
        "  else:\n",
        "    restored_samples_overlap.append(np.array(sqnc[SQNC_LENGTH//4:(SQNC_LENGTH*3)//4]))\n",
        "\n",
        "restored_samples_overlap = np.array(restored_samples_overlap).flatten()\n",
        "#print(type(restored_samples_overlap))\n",
        "print(restored_samples_overlap.shape)\n",
        "\n",
        "\"\"\"Если мы хотим произвести сравнение с каким-либо другим методом, возможно, возникнет проблема из-за разных длин файлов: текущий алгоритм отбрасывает последние сэмплы в файле чтобы достичь количества сэмплов кратного SQNC_LENGTH. Если раскомментировать вторую строку мы получим массив в котором недостающие восстановленные сэмплы заменены сэмплами исходного массива до требуемой длины, что обеспечит возможность сравнения файлов. output_path - название файла, в который будет записан вывод программы.\"\"\"\n",
        "\n",
        "#samples_restored_final = samples_restored\n",
        "#for i in range(len(samples_restored_final)):\n",
        "  #print(type(samples_restored_final),len(samples_restored_final[i]))\n",
        "import wave\n",
        "import numpy as np\n",
        "\n",
        "def write_float_samples_to_wav(samples, sample_rate, output_path):\n",
        "    \"\"\"\n",
        "    Writes floating-point audio samples to a mono 16-bit WAV file.\n",
        "\n",
        "    Parameters:\n",
        "        samples (list or np.ndarray): Array of floating-point audio samples in the range [-1.0, 1.0].\n",
        "        sample_rate (int): Sample rate of the audio in Hz (e.g., 44100).\n",
        "        output_path (str): Path to save the output WAV file.\n",
        "    \"\"\"\n",
        "    # Ensure the samples are a NumPy array\n",
        "    samples = np.array(samples, dtype=np.float32)\n",
        "\n",
        "    # Clip the samples to the range [-1.0, 1.0] to prevent overflow\n",
        "    samples = np.clip(samples, -1.0, 1.0)\n",
        "\n",
        "    # Convert to 16-bit PCM format\n",
        "    int_samples = (samples * 32767).astype(np.int16)\n",
        "\n",
        "    # Write to a WAV file\n",
        "    with wave.open(output_path, 'wb') as wav_file:\n",
        "        # Set the parameters for the WAV file\n",
        "        wav_file.setnchannels(1)  # Mono\n",
        "        wav_file.setsampwidth(2)  # 16-bit PCM\n",
        "        wav_file.setframerate(sample_rate)\n",
        "\n",
        "        # Write the audio frames\n",
        "        wav_file.writeframes(int_samples.tobytes())\n",
        "\n",
        "output_path = 'output.wav'  # Path to save the WAV file\n",
        "\n",
        "#write_float_samples_to_wav(samples_restored_final, fs, output_path)\n",
        "#print(f\"WAV file written to {output_path}\")\n",
        "restored_samples_overlap = np.array(restored_samples_overlap).flatten()\n",
        "restored_samples_overlap = np.append(np.array(samples_input_file[0:SQNC_LENGTH//4]),restored_samples_overlap)\n",
        "restored_samples_overlap = np.append(restored_samples_overlap,np.array(samples_input_file[-SQNC_LENGTH//4:]))\n",
        "write_float_samples_to_wav(restored_samples_overlap, fs, output_path)\n",
        "print(f\"WAV file written to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"clipping_interpolation_model.keras\")"
      ],
      "metadata": {
        "id": "fUdYjiGY5Jyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restored_samples_overlap = []\n",
        "overlap_input_sequences = []\n",
        "step_size = SQNC_LENGTH // 2\n",
        "j = 0\n",
        "maxv = np.max(np.array(samples_input_file))\n",
        "minv = np.min(np.array(samples_input_file))\n",
        "while j < len(samples_input_file):\n",
        "    #print(j, j+SQNC_LENGTH-1)\n",
        "    if(j+step_size < len(samples_input_file)):\n",
        "        overlap_input_sequences.append(samples_input_file[j:j+SQNC_LENGTH])\n",
        "    j += step_size\n",
        "for sqnc in overlap_input_sequences:\n",
        "  elem = np.array(sqnc)\n",
        "  elem = np.expand_dims(elem, axis=0)  # Now shape is (1, SQNC_LENGTH)\n",
        "  res = model.predict(elem,verbose=0).flatten()\n",
        "  restored_samples_overlap.append(res[SQNC_LENGTH//4:(SQNC_LENGTH*3)//4])\n",
        "  '''if(max(sqnc)>(maxv*0.95) or min(sqnc)<(minv*0.95)):\n",
        "    elem = np.array(sqnc)\n",
        "    elem = np.expand_dims(elem, axis=0)  # Now shape is (1, SQNC_LENGTH)\n",
        "    res = model.predict(elem,verbose=0).flatten()\n",
        "    #print(res[SQNC_LENGTH//4:(SQNC_LENGTH*3)//4])\n",
        "    restored_samples_overlap.append(res[SQNC_LENGTH//4:(SQNC_LENGTH*3)//4])\n",
        "  else:\n",
        "    restored_samples_overlap.append(np.array(sqnc[SQNC_LENGTH//4:(SQNC_LENGTH*3)//4]))'''\n",
        "\n",
        "restored_samples_overlap = np.array(restored_samples_overlap).flatten()\n",
        "#print(type(restored_samples_overlap))\n",
        "print(restored_samples_overlap.shape)\n",
        "\n",
        "\"\"\"Если мы хотим произвести сравнение с каким-либо другим методом, возможно, возникнет проблема из-за разных длин файлов: текущий алгоритм отбрасывает последние сэмплы в файле чтобы достичь количества сэмплов кратного SQNC_LENGTH. Если раскомментировать вторую строку мы получим массив в котором недостающие восстановленные сэмплы заменены сэмплами исходного массива до требуемой длины, что обеспечит возможность сравнения файлов. output_path - название файла, в который будет записан вывод программы.\"\"\"\n",
        "\n",
        "#samples_restored_final = samples_restored\n",
        "#for i in range(len(samples_restored_final)):\n",
        "  #print(type(samples_restored_final),len(samples_restored_final[i]))\n",
        "import wave\n",
        "import numpy as np\n",
        "\n",
        "def write_float_samples_to_wav(samples, sample_rate, output_path):\n",
        "    \"\"\"\n",
        "    Writes floating-point audio samples to a mono 16-bit WAV file.\n",
        "\n",
        "    Parameters:\n",
        "        samples (list or np.ndarray): Array of floating-point audio samples in the range [-1.0, 1.0].\n",
        "        sample_rate (int): Sample rate of the audio in Hz (e.g., 44100).\n",
        "        output_path (str): Path to save the output WAV file.\n",
        "    \"\"\"\n",
        "    # Ensure the samples are a NumPy array\n",
        "    samples = np.array(samples, dtype=np.float32)\n",
        "\n",
        "    # Clip the samples to the range [-1.0, 1.0] to prevent overflow\n",
        "    samples = np.clip(samples, -1.0, 1.0)\n",
        "\n",
        "    # Convert to 16-bit PCM format\n",
        "    int_samples = (samples * 32767).astype(np.int16)\n",
        "\n",
        "    # Write to a WAV file\n",
        "    with wave.open(output_path, 'wb') as wav_file:\n",
        "        # Set the parameters for the WAV file\n",
        "        wav_file.setnchannels(1)  # Mono\n",
        "        wav_file.setsampwidth(2)  # 16-bit PCM\n",
        "        wav_file.setframerate(sample_rate)\n",
        "\n",
        "        # Write the audio frames\n",
        "        wav_file.writeframes(int_samples.tobytes())\n",
        "\n",
        "output_path = 'output.wav'  # Path to save the WAV file\n",
        "\n",
        "#write_float_samples_to_wav(samples_restored_final, fs, output_path)\n",
        "#print(f\"WAV file written to {output_path}\")\n",
        "restored_samples_overlap = np.array(restored_samples_overlap).flatten()\n",
        "restored_samples_overlap = np.append(np.array(samples_input_file[0:SQNC_LENGTH//4]),restored_samples_overlap)\n",
        "restored_samples_overlap = np.append(restored_samples_overlap,np.array(samples_input_file[-SQNC_LENGTH//4:]))\n",
        "write_float_samples_to_wav(restored_samples_overlap, fs, output_path)\n",
        "print(f\"WAV file written to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkxm40kv5VnN",
        "outputId": "6dd9f3d6-5f23-4429-fbc5-f2e8ab67c194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(122752,)\n",
            "WAV file written to output.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sq_lngth = 256\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(sq_lngth,)),\n",
        "    STFTLayer(frame_length=FSTEP*2, frame_step=FSTEP)\n",
        "])"
      ],
      "metadata": {
        "id": "X5WV81Sy3fwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}