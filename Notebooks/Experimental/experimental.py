# -*- coding: utf-8 -*-
"""Experimental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sakgMfi7rWd_pO6fIB1zYbAVN1TlxzGk

Открываем исходный файл и файл с клиппингом.

*   wav_file_path - путь до исходного файла (без клиппинга), его данные записываются в массив samples
*   wav_file_path1 - путь до файла с клиппингом, его данные записываются в массив samples1
"""

import wave
import numpy as np

def read_wav_as_float(file_path):
    """
    Reads a WAV file and returns its samples as a list of floating-point values.

    Parameters:
        file_path (str): Path to the WAV file.

    Returns:
        list: A list of floating-point samples.
    """
    # Open the WAV file
    with wave.open(file_path, 'rb') as wav_file:
        # Get parameters
        n_channels = wav_file.getnchannels()
        sample_width = wav_file.getsampwidth()
        n_frames = wav_file.getnframes()
        frame_rate = wav_file.getframerate()

        print(f"Channels: {n_channels}, Sample Width: {sample_width}, Frame Rate: {frame_rate}, Frames: {n_frames}")

        # Read frames as bytes
        raw_data = wav_file.readframes(n_frames)

    # Determine the data type based on sample width
    dtype = {1: np.int8, 2: np.int16, 4: np.int32}.get(sample_width)
    if dtype is None:
        raise ValueError(f"Unsupported sample width: {sample_width}")

    # Convert raw bytes to numpy array
    int_data = np.frombuffer(raw_data, dtype=dtype)

    # Normalize to floating-point range [-1.0, 1.0]
    max_val = float(2 ** (8 * sample_width - 1))
    float_data = int_data / max_val

    # Handle multi-channel audio by averaging channels
    if n_channels > 1:
        float_data = float_data.reshape(-1, n_channels).mean(axis=1)

    return float_data.tolist()

# Example usage
wav_file_path = 'one_16khz.wav'  # Replace with the path to your WAV file
wav_file_path1 = 'one_c_16khz.wav'
samples = read_wav_as_float(wav_file_path)
samples1 = read_wav_as_float(wav_file_path1)

"""Функция для записи массива в файл по пути output_path."""

def write_float_samples_to_wav(samples, sample_rate, output_path):
    """
    Writes floating-point audio samples to a mono 16-bit WAV file.

    Parameters:
        samples (list or np.ndarray): Array of floating-point audio samples in the range [-1.0, 1.0].
        sample_rate (int): Sample rate of the audio in Hz (e.g., 44100).
        output_path (str): Path to save the output WAV file.
    """
    # Ensure the samples are a NumPy array
    samples = np.array(samples, dtype=np.float32)

    # Clip the samples to the range [-1.0, 1.0] to prevent overflow
    samples = np.clip(samples, -1.0, 1.0)

    # Convert to 16-bit PCM format
    int_samples = (samples * 32767).astype(np.int16)

    # Write to a WAV file
    with wave.open(output_path, 'wb') as wav_file:
        # Set the parameters for the WAV file
        wav_file.setnchannels(1)  # Mono
        wav_file.setsampwidth(2)  # 16-bit PCM
        wav_file.setframerate(sample_rate)

        # Write the audio frames
        wav_file.writeframes(int_samples.tobytes())
#import random
#for i in range(100):
#  start = random.randint(0,len(samples1))
#  samples1[start:start+15] = [1e-10]*15
#write_float_samples_to_wav(samples1, 16000, "1c16zeros.wav")

"""Разбиваем оба файла на последовательности по SQNC_LENGTH сэмплов, сэмплы исходного файла в samples_sequences, искаженного в sampeles_sequences_clipped."""

j = 0
SQNC_LENGTH = 256
samples_sequences = []
samples_sequences_clipped = []
while j < len(samples1):
    if(j+SQNC_LENGTH < len(samples1)):
        samples_sequences.append(samples[j:j+SQNC_LENGTH])
        samples_sequences_clipped.append(samples1[j:j+SQNC_LENGTH])
    j += SQNC_LENGTH

"""Заполнение массива спектрограмм сигнала. Zxx - амплитудная спектрограмма последовательностей исходного сигнала, Zxx1 - амплитудная спектрограмма последовательностей сигнала с клиппингом. phs и phs1 - соответствующие фазовые спектрограмма последовательностей"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from scipy.signal import stft, istft
Zxx = []
Zxx1 = []
phs = []
phs1 = []
fs = 16000
t = np.linspace(0, 1.0, SQNC_LENGTH, endpoint=False)  # Time vector
for i in range(len(samples_sequences_clipped)):
  signal = samples_sequences[i]
  signal1 = samples_sequences_clipped[i]
  # Parameters for STFT

  #shape of the result is (a,b) where a = frame_length//2+1 and b = ceil(N//a), N - number of samples in original sequence
  frame_length = 8  # Frame length (number of samples per frame)
  frame_step = frame_length//2  # Step between frames (overlap)

  # Compute the STFT using scipy's stft function
  f, t_stft, Z = stft(signal, fs, nperseg=frame_length, noverlap=frame_step)
  f1, t_stft1, Z1 = stft(signal1, fs, nperseg=frame_length, noverlap=frame_step)
  Zxx.append(np.abs(Z))
  Zxx1.append(np.abs(Z1))
  phs.append(tf.math.angle(Z))
  phs1.append(tf.math.angle(Z1))

"""Обучение нейросети на множестве спектрограмм сигнала. N и M - количество точек по осям частоты и времени соответственно в обучающих выборках."""

from tensorflow.keras import layers, models
from tensorflow.keras import Model
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Normalization
import numpy as np
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler
N, M = 5, 65  # Spectrogram dimensions
def scheduler(epoch, lr):
    if epoch < 50:
        return lr
    else:
        return lr * 0.95

lr_scheduler = LearningRateScheduler(scheduler)
def generate_identity_dataset(num_samples=1000):
    X = np.random.rand(num_samples, N, M)
    return X, X

def build_rnn_spectrogram_model(N, M):
    input_shape = (N, M, 1)  # Input shape for spectrograms

    input_tensor = layers.Input(shape=input_shape)
    x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(input_tensor)
    x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = layers.Reshape((N, M * 64))(x)
    x = layers.SimpleRNN(units=256, activation='relu', return_sequences=True)(x)
    x = layers.SimpleRNN(units=128, activation='relu', return_sequences=True)(x)
    x = layers.Dense(M, activation='linear')(x)
    x = layers.Reshape((N, M, 1))(x)


    # Add residual connection
    output_tensor = layers.Multiply()([input_tensor, x])
    model = Model(inputs=input_tensor, outputs=output_tensor)

    #model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mse'])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_absolute_error', metrics=['mae'])
    return model

from tensorflow.keras.callbacks import EarlyStopping


model = build_rnn_spectrogram_model(N, M)
model.summary()
early_stopping = EarlyStopping(monitor='loss', patience=40, restore_best_weights=True)
print(np.array(Zxx1).shape)
# Train model with normalized data
model.fit(np.array(Zxx1), np.array(Zxx) , epochs=1000, callbacks=[early_stopping])

"""Открытие файла который нужно восстановить и получение массива его спектрограмм. file_for_restoration_path - путь к файлу который нужно восстановить.
samples_input_sequences - массив семплов этого файла

Zyy,phsy - массивы амплитудных и фазовых спектрограмм файла соответственно
"""

file_for_restoration_path = "one_c_16khz.wav"
samples_input_file = read_wav_as_float(file_for_restoration_path)
j = 0
SQNC_LENGTH = 256
fs = 44100
samples_input_sequences = []
while j < len(samples_input_file ):
    if(j+SQNC_LENGTH < len(samples_input_file)):
        samples_input_sequences.append(samples_input_file[j:j+SQNC_LENGTH])
    j += SQNC_LENGTH
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from scipy.signal import stft, istft
import wave
# Open the WAV file
with wave.open(file_for_restoration_path, 'rb') as wav_file:
    fs = wav_file.getframerate()
Zyy = []
phsy = []
t = np.linspace(0, 1.0, SQNC_LENGTH, endpoint=False)  # Time vector
for i in range(len(samples_input_sequences)):
  signal = samples_input_sequences[i]
  # Parameters for STFT

  #shape of the result is (a,b) where a = frame_length//2+1 and b = ceil(N//a), N - number of samples in original sequence
  frame_length = 8  # Frame length (number of samples per frame)
  frame_step = frame_length//2  # Step between frames (overlap)

  # Compute the STFT using scipy's stft function
  f, t_stft, Z = stft(signal, fs, nperseg=frame_length, noverlap=frame_step)
  Zyy.append(np.abs(Z))
  phsy.append(tf.math.angle(Z))

"""Восстановление звука с помощью нейросети. samples_restored - массив восстановленных сэмплов звука."""

import cmath
import math
samples_restored = []
for i in range(len(Zyy)):
  t = np.linspace(0, 1.0, SQNC_LENGTH, endpoint=False)  # Time vector
  #shape of the result is (a,b) where a = frame_length//2+1 and b = ceil(N//a), N - number of samples in original sequence
  frame_length = 8  # Frame length (number of samples per frame)
  frame_step = 4  # Step between frames (overlap)
  spgram = np.empty((N,M))
  spgram = model.predict(Zyy[i].reshape(1,N,M)).reshape((N,M))*np.exp(1j*phsy[i].numpy())
  _, reconstructed_signal = istft(spgram, fs, nperseg=frame_length, noverlap=frame_step)
  samples_restored.append(reconstructed_signal[0:SQNC_LENGTH])

"""Если мы хотим произвести сравнение с каким-либо другим методом, возможно, возникнет проблема из-за разных длин файлов: текущий алгоритм отбрасывает последние сэмплы в файле чтобы достичь количества сэмплов кратного SQNC_LENGTH. Если раскомментировать вторую строку мы получим массив в котором недостающие восстановленные сэмплы заменены сэмплами исходного массива до требуемой длины, что обеспечит возможность сравнения файлов. output_path - название файла, в который будет записан вывод программы."""

samples_restored_final = samples_restored
samples_restored_final = np.append(np.array(samples_restored_final).flatten(),np.array(samples_input_file[len(samples_input_file)-(len(samples_input_file) % SQNC_LENGTH)::]))
import wave
import numpy as np

def write_float_samples_to_wav(samples, sample_rate, output_path):
    """
    Writes floating-point audio samples to a mono 16-bit WAV file.

    Parameters:
        samples (list or np.ndarray): Array of floating-point audio samples in the range [-1.0, 1.0].
        sample_rate (int): Sample rate of the audio in Hz (e.g., 44100).
        output_path (str): Path to save the output WAV file.
    """
    # Ensure the samples are a NumPy array
    samples = np.array(samples, dtype=np.float32)

    # Clip the samples to the range [-1.0, 1.0] to prevent overflow
    samples = np.clip(samples, -1.0, 1.0)

    # Convert to 16-bit PCM format
    int_samples = (samples * 32767).astype(np.int16)

    # Write to a WAV file
    with wave.open(output_path, 'wb') as wav_file:
        # Set the parameters for the WAV file
        wav_file.setnchannels(1)  # Mono
        wav_file.setsampwidth(2)  # 16-bit PCM
        wav_file.setframerate(sample_rate)

        # Write the audio frames
        wav_file.writeframes(int_samples.tobytes())

output_path = 'output.wav'  # Path to save the WAV file

write_float_samples_to_wav(samples_restored_final, fs, output_path)
print(f"WAV file written to {output_path}")

"""Занимательные картинки."""

import cmath
import math
for i in range(20,len(samples_input_sequences)):
  fs = 44100
  t = np.linspace(0, 1.0, SQNC_LENGTH, endpoint=False)  # Time vector
  #shape of the result is (a,b) where a = frame_length//2+1 and b = ceil(N//a), N - number of samples in original sequence
  frame_length = 8  # Frame length (number of samples per frame)
  frame_step = 4  # Step between frames (overlap)
  spgram = np.empty((N,M))
  #for j in range(N):
    #for k in range(M):
  spgram=model.predict(Zyy[i].reshape(1,N,M)).reshape((N,M))*np.exp(1j*phsy[i].numpy())
  #print(Zxx1[10]*(math.cos(phs[10])+1j*math.sin(phs[10])))
  #print(spgram[8][2])
  _, reconstructed_signal = istft(spgram, fs, nperseg=frame_length, noverlap=frame_step)

  # Plot reconstructed signal
  plt.subplot(2, 1, 2)
  #plt.plot(t, samples_sequences[i], label="Original Signal", color='blue')
  plt.plot(t, samples_input_sequences[i], label="Clipped Signal", color='green')
  plt.plot(t, samples_restored[i], label="Reconstructed Signal", color='orange')
  plt.title('Reconstructed Signal from Spectrogram')
  plt.xlabel('Time [s]')
  plt.ylabel('Amplitude')
  plt.legend()

  plt.tight_layout()
  plt.show()

print(samples_input_sequences[40])